{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bd5af7-ccb4-4fa9-a62e-374acd5e39f5",
   "metadata": {},
   "source": [
    "Apache Spark\n",
    "-----\n",
    "\n",
    "Apache Spark is an open-source, distributed computing system that provides a fast and general-purpose cluster-computing framework for big data processing. It was developed to address the limitations and challenges posed by traditional MapReduce-based systems by introducing a more flexible and efficient architecture. The goal of this assignment is to familiarize you with Apache Spark operations through practical exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86d43f-9e7c-44e8-b400-8eb173a27520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, substring, avg, mean, sum, round, count, collect_list, last, when, lower, regexp_replace, current_date, datediff, when, to_date, expr, format_number,trim, desc, first\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, BooleanType, FloatType, DoubleType, TimestampType, IntegerType, LongType\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b259bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the dataset is present (creates ./dataset/*.csv)\n",
    "from downloader import ensure_dataset\n",
    "\n",
    "ensure_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101409e-2b30-45ae-8c69-1ba65fd021ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $SPARK_MASTER_ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636a319-60c4-45e3-8f83-eb61d5ef03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(os.getenv('SPARK_MASTER_ADDRESS')).appName(\"Spark-application\").config(\"spark.ui.enabled\", \"false\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df439f-1c23-4561-8cc9-9c307f657da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $SPARK_MASTER_OOD_ADDRESS\n",
    "!echo $SPARK_WORKER1_OOD_ADDRESS\n",
    "!echo $SPARK_WORKER2_OOD_ADDRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488c0fc",
   "metadata": {},
   "source": [
    "# Note: \n",
    "\n",
    "1. We have provided many hints on which functions to use for a particular task. If you feel like you can solve it without those methods or using different methods, you can do so.\n",
    "\n",
    "2. Also, if you feel like that some columns should have been type converted or dropped and are not done during this exercise, feel free to do so. You can provide a small explanation.\n",
    "\n",
    "3. The feature columns should be the following columns. Some are already present in the dataset and some are derived below in the exercise:\n",
    "\n",
    "    'investment_rounds',\n",
    "    'invested_companies',\n",
    "    'funding_rounds',\n",
    "    'relationships',\n",
    "    'age_of_company',\n",
    "    'total_amount_raised',\n",
    "    'num_acquisitions',\n",
    "    'have_been_acquired',\n",
    "    'fin_org_financed',\n",
    "    'person_financed',\n",
    "    'startup_financed',\n",
    "    'num_products',\n",
    "    'category_code_index',\n",
    "    'country_code_index'\n",
    "\n",
    "4. If before creating feature vector, you have other columns in your dataset than mentioned above, drop them. If you want, you can include them in features, but mention it in the notebook.\n",
    "\n",
    "5. Your code should be error free as we will run each cell while grading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128988b-6bea-4c99-912e-19d6937ad9f8",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The database is composed of 5 tables containing many aspects related to the startup world from 1901-01-01 to 2014-10-01. Each table in the dataset represents a different aspect of the startup ecosystem, detailing the interactions, events, and entities involved in startup funding, growth, and development.\n",
    "\n",
    "-------\n",
    "### Objects\n",
    "\n",
    "A broad table that likely serves as a central repository for entities in the dataset, including companies, startups, and perhaps other organizational forms. It includes detailed information on each entity, such as its status, industry category, funding received, and key milestones.\n",
    "It contains 40 variables, of which the most important are name, entity_type, category_code, status, founded_at, country_code, state_code, investment_rounds, invested_companies, funding_rounds.\n",
    "\n",
    "------\n",
    "### Investments\n",
    "\n",
    "Tracks investment transactions, detailing which entities (investors) have invested in which companies or startups during particular funding rounds.\n",
    "\n",
    "\n",
    "-------\n",
    "<!-- Offices\n",
    "-----\n",
    "geographic position of main offices (both of the companies and the investment funds). -->\n",
    "\n",
    "### Funding Rounds\n",
    "\n",
    "Captures details about specific funding events where startups or companies receive investment. (funded company, date and funding type, total raised amount, number of participants)\n",
    "\n",
    "-----\n",
    "\n",
    "### Relationships\n",
    "\n",
    "Tracks the connections between individuals (people) and entities (companies, startups), including roles or positions held by individuals within companies, indicating the network of professionals in the startup ecosystem. (people, institutions, start and end date of relationship, role held)\n",
    "\n",
    "------\n",
    "### Acquisitions\n",
    "\n",
    "Details about acquisition events where one company purchases another. (acquired company, acquiring company, price and date of acquisition, payment method)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb664dbb",
   "metadata": {},
   "source": [
    "Here are some links to documentation which would be helpful for the tasks:\n",
    "\n",
    "\n",
    "Pyspark sql datframe: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\n",
    "\n",
    "\n",
    "Pyspark sql functions: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215fa48",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545dcd-0686-4424-9913-421637072ddf",
   "metadata": {},
   "source": [
    "## 1 .Read the following files to the Spark DataFrame and select given columns\n",
    "\n",
    "\n",
    "1. **objects.csv**\n",
    "\n",
    "   -  **Columns:** \"id\",\n",
    "    \"entity_type\",\n",
    "    \"entity_id\",\n",
    "    \"parent_id\",\n",
    "    \"name\",\n",
    "    \"category_code\",\n",
    "    \"status\",\n",
    "    \"founded_at\",\n",
    "    \"closed_at\",\n",
    "    \"country_code\",\n",
    "    \"state_code\",\n",
    "    \"city\",\n",
    "    \"region\",\n",
    "    \"investment_rounds\",\n",
    "    \"invested_companies\",\n",
    "    \"first_funding_at\",\n",
    "    \"last_funding_at\",\n",
    "    \"funding_rounds\",\n",
    "    \"funding_total_usd\",\n",
    "    \"participants\",\n",
    "    \"relationships\"\n",
    "  \n",
    "2. **acquisitions.csv**\n",
    "\n",
    "   - **Columns:** 'id', 'acquisition_id', 'acquiring_object_id', 'acquired_object_id',\n",
    "       'term_code', 'price_amount', 'price_currency_code', 'acquired_at'\n",
    "  \n",
    "2. **funding_rounds.csv**\n",
    "\n",
    "   - **Columns:** \"object_id\",\n",
    "    \"funded_at\",\n",
    "    \"funding_round_type\",\n",
    "    \"funding_round_code\",\n",
    "    \"raised_amount_usd\",\n",
    "    \"is_first_round\",\n",
    "    \"is_last_round\"\n",
    "    \n",
    "2. **investments.csv**\n",
    "\n",
    "   - **Columns:** 'funding_round_id', 'funded_object_id', 'investor_object_id'\n",
    "  \n",
    "2. **relationships.csv**\n",
    "\n",
    "   - **Columns:** \"id\",\n",
    "    \"person_object_id\",\n",
    "    \"relationship_object_id\",\n",
    "    \"start_at\",\n",
    "    \"end_at\",\n",
    "    \"is_past\",\n",
    "    \"title\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c13eb-f015-4ae5-89ed-c5a6a64dab2c",
   "metadata": {},
   "source": [
    "##### Read following csv files to the spark dataframe\n",
    "\n",
    "Hint: Use .select() to select the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2648b5-3ba4-41b0-80a7-3f695ce9b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#objects\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6f739-5aaf-4dfd-8741-d7802d2a0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquisitions\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0bad1-8509-4040-9fe4-0c05fbf3d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funding_rounds\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974363f4-ac3d-4551-8798-0ec6fed86a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investments\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81384a-564c-48d2-927f-26cc23e8f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relationships\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d25130",
   "metadata": {},
   "source": [
    "## 2.1 Clean objects table\n",
    "\n",
    "\n",
    "1. Type conversion: convert the datatype of given columns\n",
    "        \n",
    "        Column with dates: datetime type\n",
    "\n",
    "        investment_rounds, invested_companies, funding_rounds, funding_total_usd, milestones, relationships: Numeric\n",
    "\n",
    "\n",
    "2. Find Age of company in years.\n",
    "\n",
    "        \n",
    "        To calculate the age of a company, subtract the date in the 'founded_at' column from the date in the 'closed_at' column. If the 'closed_at' date is not provided (NULL), use the current date(today's date) instead.  If 'founded_at' date is NULL, then age of company will be NULL.\n",
    "        After the above operation fill the NULL values in 'age_of_company' column with median age.\n",
    "        Drop 'founded_at' and 'closed_at' after deriving the new column.\n",
    "\n",
    "\n",
    "3. Handle Variation. \n",
    "\n",
    "        The 'status' column in objects dataframe categorizes a company's current operational phase, such as 'operating', 'ipo', 'acquired', or 'closed', reflecting its lifecycle stage.\n",
    "        \n",
    "        To ensure uniformity in company status reporting, categorize all entries in the 'status' column into one of four standardized categories: 'operating', 'ipo', 'acquired', or 'closed'. You need to map various existing status names, which might currently reflect the same operational state in different terminology. Display the count of each status after modification. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8e208",
   "metadata": {},
   "source": [
    "Hint: Use methods withColumn, col, cast for type conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Type conversion to timestamp\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 Type conversion to integer\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae885857",
   "metadata": {},
   "source": [
    "Hint: Some function used will be when, col, datediff, currentdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Age_of_company\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412be02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Map status to predefined categories\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3c0ee",
   "metadata": {},
   "source": [
    "## 2.2 Clean Funding_rounds table\n",
    "\n",
    "1. Cleaning raised_amount_usd column:\n",
    "\n",
    "       The raised_amount_usd has values in inconsistent formatting with special characters. Remove those and create a consistent formatting of values.\n",
    "\n",
    "2. Filling NULL values in raised_amount_usd column:\n",
    "\n",
    "        Calculate the average 'raised_amount_usd' for each 'funding_round_type'. For each funding type, replace missing or null values in the raised_amount_usd column with the corresponding average amount raised for that funding type.\n",
    "\n",
    "3. Create new column 'total_amount_raised' by aggregating the 'raised_amount_usd' column based on 'object_id'.\n",
    "\n",
    "3. Display the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03109ad9",
   "metadata": {},
   "source": [
    "Hint: Use groupBy() and mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f315ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix values for column: raised_amount_usd\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate raised_amount_usd\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d31875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display dataframe\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc75078",
   "metadata": {},
   "source": [
    "## 3. Print duplicate rows count and remove those rows from each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Duplicates\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6cda06-3c70-45ae-8f04-50075decd5ca",
   "metadata": {},
   "source": [
    "## 4. Splitting the Objects Table\n",
    "\n",
    "The object dataset consists of information about startups, including details about their products and the nature of their relationships with financial organizations and persons.\n",
    "\n",
    "We would like to create different entities based on the entity_type. Using these entities we will derive new features which will be later used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311a9f8-8517-4c04-934d-c45b9383ad92",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Question: Divide the 'objects' dataset into four distinct datasets based on the 'entity_type' column. Also display count of rows for each dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd27d4",
   "metadata": {},
   "source": [
    "Hint: Use filter() method to filter rows using the given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555be683-a605-4f70-b9b8-c8843cb14302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create startups\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create financial_org\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96870ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create products\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d61faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create persons\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c060ec",
   "metadata": {},
   "source": [
    "## 5. Derive new features\n",
    "\n",
    "This part will require you to apply joins and derive aggregated features by grouping data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b78ed1",
   "metadata": {},
   "source": [
    "Hint: Use groupBy() to group and agg() to aggregate over dataframe, alias() to give new name to column, join() to join tables if required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed6ea4",
   "metadata": {},
   "source": [
    "### 5.1\n",
    "\n",
    "You are provided with dataset named 'acquisitions'.\n",
    "\n",
    "    i) The column 'acquiring_object_id' indicates the entity that has made the acquisition. Group the data based on the 'acquiring_object_id'. For each group, count the total number of acquisitions made. Rename the aggregated count to 'num_acquisitions'. \n",
    "    Your final output should be a DataFrame named 'acquiring' that lists each 'acquiring_object_id' alongside the corresponding 'num_acquisitions'. Display the dataframe.\n",
    "\n",
    " \n",
    "    ii) The column 'acquired_object_id' specifies the entity that was acquired. Group the dataset by 'acquired_object_id' to organize the data by each entity that has been acquired. Compute the count of acquisitions for each group. Rename the aggregated count to 'have_been_acquired'. \n",
    "    Your final output should be a DataFrame named 'acquired' that lists each 'acquired_object_id' alongside the corresponding 'have_been_acquired'. Display the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b23805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create num_acquisitions\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65710c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display acquiring df\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have_been_acquired\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display acquired df\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388feb31",
   "metadata": {},
   "source": [
    "### 5.2\n",
    "\n",
    "    Utilize the 'investments' and 'financial_org' datasets to identify how many times each entity has been financed by financial organizations.\n",
    "  \n",
    "    Your final output should be a DataFrame named 'finorgs' that lists each 'funded_object_id' alongside the corresponding 'fin_org_financed'. Display the dataframe.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55506d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create finorgs\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea41fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display finorgs\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a305f",
   "metadata": {},
   "source": [
    "### 5.3\n",
    "\n",
    "    Determine the number of investments made by individuals in various entities using the 'investments' and 'persons' datasets. \n",
    "\n",
    "    Your final output should be a DataFrame named 'num_persons' that lists each 'funded_object_id' alongside the corresponding 'person_financed'. Display the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create num_persons\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display num_persons\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0318e7c",
   "metadata": {},
   "source": [
    "### 5.4\n",
    "\n",
    "    Calculate how many times each entity has been financed by startups using the 'investments' and 'startups' datasets.\n",
    "\n",
    "    Your final output should be a DataFrame named 'nstartup' that lists each 'funded_object_id' alongside the corresponding 'startup_financed'. Display the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nstartup\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f2245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display nstartup\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5ae48",
   "metadata": {},
   "source": [
    "### 5.5\n",
    "\n",
    "    Determine the number of products associated with each parent entity using the 'products' dataset.\n",
    "\n",
    "      Your final output should be a DataFrame named 'nproducts' that lists each 'parent_id' alongside the corresponding 'num_products'. Display the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nproducts\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display nproducts\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a6fcc",
   "metadata": {},
   "source": [
    "## 6. Joins\n",
    "\n",
    "You will join all the tables created above and create a final dataset. The name of the final dataset will be train_data. \n",
    "\n",
    "Joins in spark: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd125dcb",
   "metadata": {},
   "source": [
    " ### 6.1\n",
    " \n",
    "    Combine the 'startups' and 'funding_rounds' datasets to analyze funding details for each startup. Your output should be a DataFrame named 'train_data'. Print columns of final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400c52f",
   "metadata": {},
   "source": [
    "### 6.2\n",
    "\n",
    "    Join the 'acquiring' and 'acquired' DataFrames with the existing 'train_data' DataFrame to integrate data on acquisitions. Print columns of final dataset. You don't require the id through which you are joining these tables. Drop those columns after joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e96bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b9c42",
   "metadata": {},
   "source": [
    "### 6.3\n",
    "\n",
    "    Join the 'train_data' DataFrame by merging it with 'finorgs', 'num_persons', 'nstartup', and 'nproducts' DataFrames based on relevant ID matches and streamline the merged dataset by removing redundant columns during each join operation. Print columns of final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81091f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae34087",
   "metadata": {},
   "source": [
    "### 6.4\n",
    "\n",
    "We have compiled a comprehensive dataset that includes information on financial organization investments, individual investments, startup investments, and product counts under each entity.\n",
    "\n",
    "    Display the count of train_data. Also, display the schema of train dataset i.e the column names with corresponding data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9796c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f10f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to remove unecessary columns:\n",
    "columns_to_drop = ['parent_id', 'entity_id', 'closed_at', 'funded_object_id']\n",
    "train_data = train_data.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c9957",
   "metadata": {},
   "source": [
    "### 6.5 Converting datatypes of feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1bb48",
   "metadata": {},
   "source": [
    "Convert all the numeric data columns to float and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95649c-e3ef-4cc8-ad6f-15b4664b9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Columns to convert to float\n",
    "columns_to_convert = [\n",
    "    'investment_rounds',\n",
    "    'invested_companies',\n",
    "    'funding_rounds',\n",
    "    'relationships',\n",
    "    'total_amount_raised',\n",
    "    'num_acquisitions',\n",
    "    'fin_org_financed',\n",
    "    'num_products',\n",
    "    'startup_financed',\n",
    "    'person_financed',\n",
    "    'funding_total_usd',\n",
    "    'participants',\n",
    "    'age_of_company',\n",
    "    'raised_amount_usd',\n",
    "    'have_been_acquired'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfdffc-d44f-419c-9088-c2490d19af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert specified columns to float\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a6a9e",
   "metadata": {},
   "source": [
    "### 6.6 Transform the 'id' column in the 'train_data' DataFrame by removing any 'c:' prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e823bed-93b6-456f-ad34-171562ff0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "columns_to_convert = ['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e181ba-3b02-4b5c-8d22-f0568a3a4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346573a",
   "metadata": {},
   "source": [
    "## Q7. Spark Mlib: Classification\n",
    "\n",
    "MLlib(Main Guide): https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "MLlib (DataFrame-based): https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbea35",
   "metadata": {},
   "source": [
    "\n",
    "In this part, you will perform multiclass classification task using Apache Spark's scalable machine learning library(Spark Mlib).\n",
    "\n",
    "Predict the status of startups by performing a classification analysis on the categorical dependent variable 'Status'\n",
    "\n",
    "The dependent variable is a categorical one, made up of 4 non-orderable levels, indicating the STATUS of each startup. These levels are:\n",
    "\n",
    "    CLOSED : failed startup\n",
    "\n",
    "    ACQUIRED : acquired startup\n",
    "\n",
    "    IPO : listed startup\n",
    "\n",
    "    OPERATING : startup not acquired or listed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df12644",
   "metadata": {},
   "source": [
    "### 7.1 Fill Missing Values for categorical columns\n",
    "\n",
    "    Perform Mode imputation for categorical columns like 'category_code', 'country_code'. \n",
    "\n",
    "    Mode imputation: Replace missing values with the mode (the most frequently occurring value) of the column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ef23e",
   "metadata": {},
   "source": [
    "Hint: Order your column in descending order according to frequency and get the first value.\n",
    "Methods Used: groupBy(), orderBy(), count(), desc(), first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b26c3",
   "metadata": {},
   "source": [
    "### 7.2 Convert categorical columns to numeric using String Indexer\n",
    "\n",
    "    String Indexer assigns a unique integer based on label frequencies, with the most frequent label getting index 0.\n",
    "\n",
    "StringIndexer: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28258a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've renamed the original columns, then drop them\n",
    "train_data = train_data.drop(*categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b04c3",
   "metadata": {},
   "source": [
    " ### 7.3 Fill the missing values for numerical columns\n",
    "\n",
    "    First identify the numerical columns. Use the imputer method to fill the missing values using 'mean' strategy for these columns.\n",
    "\n",
    "Imputer(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd8ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here \n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns\n",
    "columns_to_drop = [\"id\", \"participants\", \"is_first_round\", \"is_last_round\", \"acquiring_object_id\", \"acquired_object_id\", \"funding_round_type\", \"funded_at\", \"funding_round_code\", \"raised_amount_usd\",\"object_id\"]\n",
    "train_data = train_data.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e305179",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc26be",
   "metadata": {},
   "source": [
    "### 7.4 Feature vector\n",
    "\n",
    "    \n",
    "    Use Pyspark ML's VectorAssembler() which is a feature transformer to convert the feature columns to single column vector. Index the target variable 'status'.\n",
    "\n",
    "VectorAssembler(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8634a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1d946",
   "metadata": {},
   "source": [
    "### 7.5 Model selection with 5-fold cross-validation (Random Forest vs Logistic Regression)\n",
    "\n",
    "Split the dataset into **train/test**. On the **training split only**, run **5-fold cross-validation** (Spark ML `CrossValidator`) for **two models** and pick the best model/hyperparameters based on the CV metric. Then evaluate the selected best model on the held-out **test** split (no CV-on-test) and report test accuracy.\n",
    "\n",
    "RandomForest(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.\n",
    "\n",
    "LogisticRegression(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html\n",
    "\n",
    "CrossValidator(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html\n",
    "\n",
    "ParamGridBuilder(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html\n",
    "\n",
    "Pipeline(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html\n",
    "\n",
    "MulticlassClassificationEvaluator(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a731383-ab38-4ac1-83df-ea3c376b98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742de2a0",
   "metadata": {},
   "source": [
    "### 7.6 Evaluation\n",
    "\n",
    "    Evaluate the performance of a machine learning model on a multi-class classification problem. Print out precision and recall for each class identified in the model's predictions.\n",
    "\n",
    "MulticlassMetrics(): https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.MulticlassMetrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e65bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code starts here\n",
    "\n",
    "# TODO: Your code ends here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aec82-72f7-4aa0-a08c-2b129bf63d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
